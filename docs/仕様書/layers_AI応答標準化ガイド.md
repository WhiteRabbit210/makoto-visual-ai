# layers_AIå¿œç­”æ¨™æº–åŒ–ã‚¬ã‚¤ãƒ‰

## ç›®æ¬¡

1. [æ¦‚è¦](#æ¦‚è¦)
2. [ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£](#ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£)
3. [çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹](#çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹)
4. [ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å®Ÿè£…](#ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å®Ÿè£…)
5. [Function Calling](#function-calling)
6. [ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†](#ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†)
7. [ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°](#ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°)
8. [ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–](#ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–)
9. [ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç®¡ç†](#ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç®¡ç†)
10. [ãƒ¢ãƒ‡ãƒ«ç®¡ç†](#ãƒ¢ãƒ‡ãƒ«ç®¡ç†)
11. [ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†](#ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†)
12. [ãƒ†ã‚¹ãƒˆæˆ¦ç•¥](#ãƒ†ã‚¹ãƒˆæˆ¦ç•¥)

## æ¦‚è¦

AIå¿œç­”æ¨™æº–åŒ–ãƒ¬ã‚¤ãƒ¤ãƒ¼ã¯ã€ç•°ãªã‚‹LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ï¼ˆOpenAIã€Azure OpenAIã€AWS Bedrockã€Anthropic Claudeç­‰ï¼‰ã‚’çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã§æ“ä½œã—ã€ä¸€è²«ã—ãŸå¿œç­”å½¢å¼ã‚’æä¾›ã™ã‚‹ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã§ã™ã€‚

### ä¸»è¦ç›®çš„

1. **ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ç‹¬ç«‹æ€§**: ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‹ã‚‰åˆ†é›¢
2. **å¿œç­”å½¢å¼çµ±ä¸€**: ç•°ãªã‚‹ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®å¿œç­”ã‚’çµ±ä¸€å½¢å¼ã«å¤‰æ›
3. **Function Callingæ¨™æº–åŒ–**: å„ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®Function Callingå®Ÿè£…ã‚’çµ±ä¸€
4. **ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œ**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ã®æ¨™æº–åŒ–
5. **ã‚¨ãƒ©ãƒ¼å‡¦ç†çµ±ä¸€**: ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å›ºæœ‰ã®ã‚¨ãƒ©ãƒ¼ã‚’çµ±ä¸€å½¢å¼ã«å¤‰æ›

## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

### ã‚·ã‚¹ãƒ†ãƒ æ§‹æˆå›³

```mermaid
graph TD
    %% ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å±¤
    App[ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³]
    App --> AIClient[AI Client Manager]
    
    %% æŠ½è±¡åŒ–å±¤
    AIClient --> BaseClient[Base AI Client]
    BaseClient --> ResponseParser[Response Parser]
    BaseClient --> StreamParser[Stream Parser]
    BaseClient --> FunctionCaller[Function Caller]
    
    %% ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å®Ÿè£…å±¤
    BaseClient --> OpenAIClient[OpenAI Client]
    BaseClient --> AzureClient[Azure OpenAI Client]
    BaseClient --> BedrockClient[Bedrock Client]
    BaseClient --> ClaudeClient[Claude Client]
    
    %% LLMã‚µãƒ¼ãƒ“ã‚¹å±¤
    OpenAIClient --> OpenAI[OpenAI API]
    AzureClient --> AzureOpenAI[Azure OpenAI]
    BedrockClient --> Bedrock[AWS Bedrock]
    ClaudeClient --> Claude[Anthropic Claude]
    
    %% å…±é€šæ©Ÿèƒ½
    BaseClient --> RateLimiter[Rate Limiter]
    BaseClient --> TokenCounter[Token Counter]
    BaseClient --> PromptManager[Prompt Manager]
    BaseClient --> MetricsCollector[Metrics Collector]
```

### ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆè²¬å‹™

| ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ | è²¬å‹™ |
|------------|------|
| AI Client Manager | ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼é¸æŠã¨ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆç®¡ç† |
| Base AI Client | å…±é€šã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã¨åŸºæœ¬å®Ÿè£… |
| Provider Clients | å„ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å›ºæœ‰ã®å®Ÿè£… |
| Response Parser | å¿œç­”å½¢å¼ã®çµ±ä¸€åŒ– |
| Stream Parser | ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ã®è§£æ |
| Function Caller | Function Calling ã®å®Ÿè£… |
| Rate Limiter | ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã®ç®¡ç† |
| Token Counter | ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®è¨ˆç®— |
| Prompt Manager | ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç®¡ç† |
| Metrics Collector | ä½¿ç”¨çŠ¶æ³ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›† |

## çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹

### åŸºæœ¬ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹å®šç¾©

```python
from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional, AsyncIterator, Union
from dataclasses import dataclass
from enum import Enum
from datetime import datetime
import json

class MessageRole(Enum):
    """ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ­ãƒ¼ãƒ«"""
    SYSTEM = "system"
    USER = "user"
    ASSISTANT = "assistant"
    FUNCTION = "function"
    TOOL = "tool"

class EmotionType(Enum):
    """æ„Ÿæƒ…ã‚¿ã‚¤ãƒ—ï¼ˆMAKOTOç”¨ï¼‰"""
    NEUTRAL = "neutral"
    HAPPY = "happy"
    SAD = "sad"
    ANGRY = "angry"
    SURPRISED = "surprised"
    CONFUSED = "confused"
    THINKING = "thinking"

class ResponseFormat(Enum):
    """å¿œç­”ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    TEXT = "text"
    JSON = "json_object"
    STRUCTURED = "structured"

@dataclass
class Message:
    """çµ±ä¸€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å½¢å¼"""
    role: MessageRole
    content: str
    name: Optional[str] = None
    function_call: Optional[Dict[str, Any]] = None
    tool_calls: Optional[List[Dict[str, Any]]] = None
    metadata: Dict[str, Any] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """è¾æ›¸å½¢å¼ã«å¤‰æ›"""
        data = {
            "role": self.role.value,
            "content": self.content
        }
        if self.name:
            data["name"] = self.name
        if self.function_call:
            data["function_call"] = self.function_call
        if self.tool_calls:
            data["tool_calls"] = self.tool_calls
        if self.metadata:
            data["metadata"] = self.metadata
        return data

@dataclass
class ChatCompletionRequest:
    """ãƒãƒ£ãƒƒãƒˆå®Œäº†ãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
    messages: List[Message]
    model: str
    temperature: float = 0.7
    max_tokens: Optional[int] = None
    top_p: float = 1.0
    frequency_penalty: float = 0.0
    presence_penalty: float = 0.0
    stop: Optional[List[str]] = None
    stream: bool = False
    response_format: ResponseFormat = ResponseFormat.TEXT
    functions: Optional[List[Dict[str, Any]]] = None
    function_call: Optional[Union[str, Dict[str, Any]]] = None
    tools: Optional[List[Dict[str, Any]]] = None
    tool_choice: Optional[Union[str, Dict[str, Any]]] = None
    user: Optional[str] = None
    seed: Optional[int] = None
    metadata: Dict[str, Any] = None

@dataclass
class ChatCompletionResponse:
    """çµ±ä¸€ãƒãƒ£ãƒƒãƒˆå®Œäº†å¿œç­”"""
    id: str
    model: str
    created: datetime
    choices: List['Choice']
    usage: 'Usage'
    emotion: Optional[EmotionType] = None
    metadata: Dict[str, Any] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """è¾æ›¸å½¢å¼ã«å¤‰æ›"""
        return {
            "id": self.id,
            "model": self.model,
            "created": self.created.isoformat(),
            "choices": [c.to_dict() for c in self.choices],
            "usage": self.usage.to_dict(),
            "emotion": self.emotion.value if self.emotion else None,
            "metadata": self.metadata
        }

@dataclass
class Choice:
    """å¿œç­”é¸æŠè‚¢"""
    index: int
    message: Message
    finish_reason: str
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "index": self.index,
            "message": self.message.to_dict(),
            "finish_reason": self.finish_reason
        }

@dataclass
class Usage:
    """ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡"""
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "prompt_tokens": self.prompt_tokens,
            "completion_tokens": self.completion_tokens,
            "total_tokens": self.total_tokens
        }

@dataclass
class StreamChunk:
    """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒãƒ£ãƒ³ã‚¯"""
    id: str
    model: str
    created: datetime
    choices: List['StreamChoice']
    usage: Optional[Usage] = None
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "id": self.id,
            "model": self.model,
            "created": self.created.isoformat(),
            "choices": [c.to_dict() for c in self.choices],
            "usage": self.usage.to_dict() if self.usage else None
        }

@dataclass
class StreamChoice:
    """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é¸æŠè‚¢"""
    index: int
    delta: 'Delta'
    finish_reason: Optional[str] = None
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "index": self.index,
            "delta": self.delta.to_dict(),
            "finish_reason": self.finish_reason
        }

@dataclass
class Delta:
    """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ‡ãƒ«ã‚¿"""
    role: Optional[MessageRole] = None
    content: Optional[str] = None
    function_call: Optional[Dict[str, Any]] = None
    tool_calls: Optional[List[Dict[str, Any]]] = None
    
    def to_dict(self) -> Dict[str, Any]:
        data = {}
        if self.role:
            data["role"] = self.role.value
        if self.content:
            data["content"] = self.content
        if self.function_call:
            data["function_call"] = self.function_call
        if self.tool_calls:
            data["tool_calls"] = self.tool_calls
        return data
```

### AIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåŸºåº•ã‚¯ãƒ©ã‚¹

```python
class AIClientBase(ABC):
    """AIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåŸºåº•ã‚¯ãƒ©ã‚¹"""
    
    def __init__(
        self,
        api_key: Optional[str] = None,
        organization: Optional[str] = None,
        base_url: Optional[str] = None,
        timeout: int = 30,
        max_retries: int = 3,
        rate_limiter: Optional['RateLimiter'] = None
    ):
        self.api_key = api_key
        self.organization = organization
        self.base_url = base_url
        self.timeout = timeout
        self.max_retries = max_retries
        self.rate_limiter = rate_limiter or RateLimiter()
        self.metrics_collector = MetricsCollector()
    
    @abstractmethod
    async def create_chat_completion(
        self,
        request: ChatCompletionRequest
    ) -> ChatCompletionResponse:
        """ãƒãƒ£ãƒƒãƒˆå®Œäº†ã‚’ä½œæˆ"""
        pass
    
    @abstractmethod
    async def create_chat_completion_stream(
        self,
        request: ChatCompletionRequest
    ) -> AsyncIterator[StreamChunk]:
        """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒãƒ£ãƒƒãƒˆå®Œäº†ã‚’ä½œæˆ"""
        pass
    
    @abstractmethod
    async def count_tokens(
        self,
        messages: List[Message],
        model: str
    ) -> int:
        """ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆç®—"""
        pass
    
    async def create_completion_with_retry(
        self,
        request: ChatCompletionRequest
    ) -> ChatCompletionResponse:
        """ãƒªãƒˆãƒ©ã‚¤ä»˜ããƒãƒ£ãƒƒãƒˆå®Œäº†"""
        retry_count = 0
        last_error = None
        
        while retry_count < self.max_retries:
            try:
                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒã‚§ãƒƒã‚¯
                await self.rate_limiter.wait_if_needed()
                
                # APIå‘¼ã³å‡ºã—
                response = await self.create_chat_completion(request)
                
                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
                await self.metrics_collector.record_request(
                    provider=self.__class__.__name__,
                    model=request.model,
                    tokens=response.usage.total_tokens,
                    latency=0  # å®Ÿè£…ã§è¨ˆæ¸¬
                )
                
                return response
                
            except RateLimitError as e:
                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼
                retry_count += 1
                wait_time = self.calculate_backoff(retry_count)
                await asyncio.sleep(wait_time)
                last_error = e
                
            except APIError as e:
                # API ã‚¨ãƒ©ãƒ¼
                if e.is_retryable:
                    retry_count += 1
                    wait_time = self.calculate_backoff(retry_count)
                    await asyncio.sleep(wait_time)
                    last_error = e
                else:
                    raise
        
        raise MaxRetriesExceeded(f"æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°ã‚’è¶…ãˆã¾ã—ãŸ: {last_error}")
    
    def calculate_backoff(self, retry_count: int) -> float:
        """ãƒãƒƒã‚¯ã‚ªãƒ•æ™‚é–“ã‚’è¨ˆç®—"""
        return min(2 ** retry_count, 60)  # æœ€å¤§60ç§’
```

## ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å®Ÿè£…

### OpenAIå®Ÿè£…

```python
import openai
from openai import AsyncOpenAI
import tiktoken

class OpenAIClient(AIClientBase):
    """OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå®Ÿè£…"""
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.client = AsyncOpenAI(
            api_key=self.api_key,
            organization=self.organization,
            base_url=self.base_url,
            timeout=self.timeout,
            max_retries=0  # ç‹¬è‡ªãƒªãƒˆãƒ©ã‚¤ã‚’ä½¿ç”¨
        )
    
    async def create_chat_completion(
        self,
        request: ChatCompletionRequest
    ) -> ChatCompletionResponse:
        """OpenAI ãƒãƒ£ãƒƒãƒˆå®Œäº†"""
        try:
            # ãƒªã‚¯ã‚¨ã‚¹ãƒˆå¤‰æ›
            openai_request = self._convert_request(request)
            
            # APIå‘¼ã³å‡ºã—
            response = await self.client.chat.completions.create(**openai_request)
            
            # å¿œç­”å¤‰æ›
            return self._convert_response(response)
            
        except openai.RateLimitError as e:
            raise RateLimitError(str(e))
        except openai.APIError as e:
            raise APIError(str(e), is_retryable=True)
        except Exception as e:
            raise APIError(str(e), is_retryable=False)
    
    async def create_chat_completion_stream(
        self,
        request: ChatCompletionRequest
    ) -> AsyncIterator[StreamChunk]:
        """OpenAI ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°"""
        try:
            # ãƒªã‚¯ã‚¨ã‚¹ãƒˆå¤‰æ›
            openai_request = self._convert_request(request)
            openai_request['stream'] = True
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‘¼ã³å‡ºã—
            stream = await self.client.chat.completions.create(**openai_request)
            
            # ãƒãƒ£ãƒ³ã‚¯å¤‰æ›
            async for chunk in stream:
                yield self._convert_stream_chunk(chunk)
                
        except Exception as e:
            raise APIError(str(e))
    
    async def count_tokens(
        self,
        messages: List[Message],
        model: str
    ) -> int:
        """ãƒˆãƒ¼ã‚¯ãƒ³æ•°è¨ˆç®—ï¼ˆtiktokenä½¿ç”¨ï¼‰"""
        try:
            encoding = tiktoken.encoding_for_model(model)
        except KeyError:
            encoding = tiktoken.get_encoding("cl100k_base")
        
        tokens = 0
        for message in messages:
            tokens += 3  # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ˜ãƒƒãƒ€ãƒ¼
            tokens += len(encoding.encode(message.content))
            if message.name:
                tokens += len(encoding.encode(message.name))
        
        tokens += 3  # å¿œç­”ãƒ—ãƒ©ã‚¤ãƒŸãƒ³ã‚°
        return tokens
    
    def _convert_request(self, request: ChatCompletionRequest) -> Dict[str, Any]:
        """çµ±ä¸€å½¢å¼ã‹ã‚‰OpenAIå½¢å¼ã¸ã®å¤‰æ›"""
        openai_request = {
            "model": request.model,
            "messages": [self._convert_message(m) for m in request.messages],
            "temperature": request.temperature,
            "top_p": request.top_p,
            "frequency_penalty": request.frequency_penalty,
            "presence_penalty": request.presence_penalty
        }
        
        if request.max_tokens:
            openai_request["max_tokens"] = request.max_tokens
        if request.stop:
            openai_request["stop"] = request.stop
        if request.response_format == ResponseFormat.JSON:
            openai_request["response_format"] = {"type": "json_object"}
        if request.functions:
            openai_request["functions"] = request.functions
        if request.function_call:
            openai_request["function_call"] = request.function_call
        if request.tools:
            openai_request["tools"] = request.tools
        if request.tool_choice:
            openai_request["tool_choice"] = request.tool_choice
        if request.user:
            openai_request["user"] = request.user
        if request.seed:
            openai_request["seed"] = request.seed
        
        return openai_request
    
    def _convert_message(self, message: Message) -> Dict[str, Any]:
        """ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å¤‰æ›"""
        openai_message = {
            "role": message.role.value,
            "content": message.content
        }
        if message.name:
            openai_message["name"] = message.name
        if message.function_call:
            openai_message["function_call"] = message.function_call
        if message.tool_calls:
            openai_message["tool_calls"] = message.tool_calls
        return openai_message
    
    def _convert_response(self, response: Any) -> ChatCompletionResponse:
        """OpenAIå¿œç­”ã‹ã‚‰çµ±ä¸€å½¢å¼ã¸ã®å¤‰æ›"""
        choices = []
        for choice in response.choices:
            message = Message(
                role=MessageRole(choice.message.role),
                content=choice.message.content or "",
                function_call=choice.message.function_call if hasattr(choice.message, 'function_call') else None,
                tool_calls=choice.message.tool_calls if hasattr(choice.message, 'tool_calls') else None
            )
            choices.append(Choice(
                index=choice.index,
                message=message,
                finish_reason=choice.finish_reason
            ))
        
        usage = Usage(
            prompt_tokens=response.usage.prompt_tokens,
            completion_tokens=response.usage.completion_tokens,
            total_tokens=response.usage.total_tokens
        )
        
        # æ„Ÿæƒ…åˆ†æï¼ˆMAKOTOç”¨ï¼‰
        emotion = self._extract_emotion(choices[0].message.content if choices else "")
        
        return ChatCompletionResponse(
            id=response.id,
            model=response.model,
            created=datetime.fromtimestamp(response.created),
            choices=choices,
            usage=usage,
            emotion=emotion
        )
    
    def _convert_stream_chunk(self, chunk: Any) -> StreamChunk:
        """ã‚¹ãƒˆãƒªãƒ¼ãƒ ãƒãƒ£ãƒ³ã‚¯å¤‰æ›"""
        choices = []
        for choice in chunk.choices:
            delta = Delta(
                role=MessageRole(choice.delta.role) if choice.delta.role else None,
                content=choice.delta.content if hasattr(choice.delta, 'content') else None,
                function_call=choice.delta.function_call if hasattr(choice.delta, 'function_call') else None,
                tool_calls=choice.delta.tool_calls if hasattr(choice.delta, 'tool_calls') else None
            )
            choices.append(StreamChoice(
                index=choice.index,
                delta=delta,
                finish_reason=choice.finish_reason
            ))
        
        return StreamChunk(
            id=chunk.id,
            model=chunk.model,
            created=datetime.fromtimestamp(chunk.created),
            choices=choices
        )
    
    def _extract_emotion(self, content: str) -> Optional[EmotionType]:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰æ„Ÿæƒ…ã‚’æŠ½å‡ºï¼ˆç°¡æ˜“å®Ÿè£…ï¼‰"""
        # å®Ÿéš›ã«ã¯ã‚ˆã‚Šé«˜åº¦ãªæ„Ÿæƒ…åˆ†æã‚’å®Ÿè£…
        emotion_keywords = {
            EmotionType.HAPPY: ["å¬‰ã—ã„", "æ¥½ã—ã„", "ğŸ˜Š", "ğŸ˜„"],
            EmotionType.SAD: ["æ‚²ã—ã„", "è¾›ã„", "ğŸ˜¢", "ğŸ˜”"],
            EmotionType.ANGRY: ["æ€’ã‚Š", "è…¹ç«‹ã¤", "ğŸ˜ ", "ğŸ˜¡"],
            EmotionType.SURPRISED: ["é©šã", "ã³ã£ãã‚Š", "ğŸ˜®", "ğŸ˜²"],
            EmotionType.CONFUSED: ["å›°æƒ‘", "åˆ†ã‹ã‚‰ãªã„", "ğŸ˜•", "ğŸ¤”"],
            EmotionType.THINKING: ["è€ƒãˆ", "æ€æ¡ˆ", "ğŸ¤”", "ğŸ’­"]
        }
        
        for emotion, keywords in emotion_keywords.items():
            if any(keyword in content for keyword in keywords):
                return emotion
        
        return EmotionType.NEUTRAL
```

### Azure OpenAIå®Ÿè£…

```python
from azure.identity import DefaultAzureCredential, ClientSecretCredential
import httpx

class AzureOpenAIClient(OpenAIClient):
    """Azure OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå®Ÿè£…"""
    
    def __init__(
        self,
        azure_endpoint: str,
        azure_deployment: str,
        api_version: str = "2024-02-15-preview",
        azure_ad_token: Optional[str] = None,
        azure_ad_token_provider: Optional[Any] = None,
        **kwargs
    ):
        # Azureå›ºæœ‰ã®è¨­å®š
        self.azure_endpoint = azure_endpoint
        self.azure_deployment = azure_deployment
        self.api_version = api_version
        
        # Azure ADèªè¨¼
        if azure_ad_token_provider:
            kwargs['api_key'] = None  # ADãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½¿ç”¨
        
        # åŸºåº•ã‚¯ãƒ©ã‚¹åˆæœŸåŒ–
        super().__init__(**kwargs)
        
        # Azure OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆä½œæˆ
        from openai import AsyncAzureOpenAI
        self.client = AsyncAzureOpenAI(
            azure_endpoint=azure_endpoint,
            azure_deployment=azure_deployment,
            api_version=api_version,
            azure_ad_token=azure_ad_token,
            azure_ad_token_provider=azure_ad_token_provider,
            api_key=kwargs.get('api_key'),
            timeout=self.timeout,
            max_retries=0
        )
    
    def _convert_request(self, request: ChatCompletionRequest) -> Dict[str, Any]:
        """Azure OpenAIç”¨ãƒªã‚¯ã‚¨ã‚¹ãƒˆå¤‰æ›"""
        # åŸºæœ¬å¤‰æ›
        azure_request = super()._convert_request(request)
        
        # Azureå›ºæœ‰: ãƒ¢ãƒ‡ãƒ«åã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆåã«ç½®æ›
        azure_request["model"] = self.azure_deployment
        
        # Azureå›ºæœ‰ã®è¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        if hasattr(request, 'azure_extensions'):
            azure_request["dataSources"] = request.azure_extensions.get('data_sources', [])
        
        return azure_request
```

### AWS Bedrockå®Ÿè£…

```python
import boto3
import json
from botocore.exceptions import ClientError

class BedrockClient(AIClientBase):
    """AWS Bedrock ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå®Ÿè£…"""
    
    def __init__(
        self,
        region_name: str = "us-east-1",
        model_id: str = "anthropic.claude-3-sonnet-20240229-v1:0",
        **kwargs
    ):
        super().__init__(**kwargs)
        self.region_name = region_name
        self.model_id = model_id
        self.bedrock_runtime = boto3.client(
            'bedrock-runtime',
            region_name=region_name
        )
    
    async def create_chat_completion(
        self,
        request: ChatCompletionRequest
    ) -> ChatCompletionResponse:
        """Bedrock ãƒãƒ£ãƒƒãƒˆå®Œäº†"""
        try:
            # ãƒ¢ãƒ‡ãƒ«åˆ¥ã®ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ä½œæˆ
            if "claude" in self.model_id:
                payload = self._create_claude_payload(request)
            elif "llama" in self.model_id:
                payload = self._create_llama_payload(request)
            elif "titan" in self.model_id:
                payload = self._create_titan_payload(request)
            else:
                raise ValueError(f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ¢ãƒ‡ãƒ«: {self.model_id}")
            
            # åŒæœŸAPIã‚’éåŒæœŸã§å®Ÿè¡Œ
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: self.bedrock_runtime.invoke_model(
                    modelId=self.model_id,
                    body=json.dumps(payload),
                    contentType='application/json'
                )
            )
            
            # å¿œç­”è§£æ
            response_body = json.loads(response['body'].read())
            return self._parse_bedrock_response(response_body, request)
            
        except ClientError as e:
            if e.response['Error']['Code'] == 'ThrottlingException':
                raise RateLimitError(str(e))
            raise APIError(str(e))
    
    async def create_chat_completion_stream(
        self,
        request: ChatCompletionRequest
    ) -> AsyncIterator[StreamChunk]:
        """Bedrock ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°"""
        try:
            # ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ä½œæˆ
            if "claude" in self.model_id:
                payload = self._create_claude_payload(request)
            else:
                raise ValueError(f"ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°éå¯¾å¿œ: {self.model_id}")
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‘¼ã³å‡ºã—
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: self.bedrock_runtime.invoke_model_with_response_stream(
                    modelId=self.model_id,
                    body=json.dumps(payload),
                    contentType='application/json'
                )
            )
            
            # ã‚¤ãƒ™ãƒ³ãƒˆã‚¹ãƒˆãƒªãƒ¼ãƒ å‡¦ç†
            for event in response['body']:
                chunk = json.loads(event['chunk']['bytes'])
                yield self._parse_bedrock_stream_chunk(chunk)
                
        except Exception as e:
            raise APIError(str(e))
    
    def _create_claude_payload(self, request: ChatCompletionRequest) -> Dict[str, Any]:
        """Claudeç”¨ãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ä½œæˆ"""
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å¤‰æ›
        messages = []
        system_prompt = None
        
        for msg in request.messages:
            if msg.role == MessageRole.SYSTEM:
                system_prompt = msg.content
            else:
                messages.append({
                    "role": "user" if msg.role == MessageRole.USER else "assistant",
                    "content": msg.content
                })
        
        payload = {
            "anthropic_version": "bedrock-2023-05-31",
            "messages": messages,
            "max_tokens": request.max_tokens or 1000,
            "temperature": request.temperature,
            "top_p": request.top_p
        }
        
        if system_prompt:
            payload["system"] = system_prompt
        
        if request.stop:
            payload["stop_sequences"] = request.stop
        
        return payload
    
    def _parse_bedrock_response(
        self,
        response_body: Dict[str, Any],
        request: ChatCompletionRequest
    ) -> ChatCompletionResponse:
        """Bedrockå¿œç­”è§£æ"""
        # Claudeå¿œç­”ã®å ´åˆ
        if "content" in response_body:
            content = response_body["content"][0]["text"] if isinstance(response_body["content"], list) else response_body["content"]
            
            message = Message(
                role=MessageRole.ASSISTANT,
                content=content
            )
            
            choice = Choice(
                index=0,
                message=message,
                finish_reason=response_body.get("stop_reason", "stop")
            )
            
            # ä½¿ç”¨é‡è¨ˆç®—ï¼ˆæ¦‚ç®—ï¼‰
            usage = Usage(
                prompt_tokens=response_body.get("usage", {}).get("input_tokens", 0),
                completion_tokens=response_body.get("usage", {}).get("output_tokens", 0),
                total_tokens=response_body.get("usage", {}).get("total_tokens", 0)
            )
            
            return ChatCompletionResponse(
                id=response_body.get("id", f"bedrock-{datetime.utcnow().timestamp()}"),
                model=self.model_id,
                created=datetime.utcnow(),
                choices=[choice],
                usage=usage
            )
        
        raise ValueError("å¿œç­”è§£æã‚¨ãƒ©ãƒ¼")
    
    async def count_tokens(
        self,
        messages: List[Message],
        model: str
    ) -> int:
        """ãƒˆãƒ¼ã‚¯ãƒ³æ•°è¨ˆç®—ï¼ˆæ¦‚ç®—ï¼‰"""
        # Bedrockã¯æ­£ç¢ºãªãƒˆãƒ¼ã‚¯ãƒ³æ•°è¨ˆç®—APIãŒãªã„ãŸã‚æ¦‚ç®—
        total_chars = sum(len(m.content) for m in messages)
        # æ–‡å­—æ•°ã‹ã‚‰æ¦‚ç®—ï¼ˆæ—¥æœ¬èªã¯1æ–‡å­—â‰’2ãƒˆãƒ¼ã‚¯ãƒ³ã€è‹±èªã¯4æ–‡å­—â‰’1ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰
        return int(total_chars * 0.5)
```

## Function Calling

### Function Callingå®Ÿè£…

```python
@dataclass
class FunctionDefinition:
    """é–¢æ•°å®šç¾©"""
    name: str
    description: str
    parameters: Dict[str, Any]  # JSON Schema
    
    def to_openai_format(self) -> Dict[str, Any]:
        """OpenAIå½¢å¼ã«å¤‰æ›"""
        return {
            "name": self.name,
            "description": self.description,
            "parameters": self.parameters
        }
    
    def to_claude_format(self) -> Dict[str, Any]:
        """Claudeå½¢å¼ã«å¤‰æ›"""
        return {
            "name": self.name,
            "description": self.description,
            "input_schema": self.parameters
        }

class FunctionCaller:
    """Function Callingå®Ÿè£…"""
    
    def __init__(self):
        self.functions: Dict[str, FunctionDefinition] = {}
        self.handlers: Dict[str, Callable] = {}
    
    def register_function(
        self,
        definition: FunctionDefinition,
        handler: Callable
    ):
        """é–¢æ•°ç™»éŒ²"""
        self.functions[definition.name] = definition
        self.handlers[definition.name] = handler
    
    async def execute_function_call(
        self,
        function_name: str,
        arguments: Dict[str, Any]
    ) -> Any:
        """é–¢æ•°å®Ÿè¡Œ"""
        if function_name not in self.handlers:
            raise ValueError(f"æœªç™»éŒ²ã®é–¢æ•°: {function_name}")
        
        handler = self.handlers[function_name]
        
        # éåŒæœŸé–¢æ•°ã®å ´åˆ
        if asyncio.iscoroutinefunction(handler):
            return await handler(**arguments)
        else:
            # åŒæœŸé–¢æ•°ã‚’éåŒæœŸã§å®Ÿè¡Œ
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(None, handler, **arguments)
    
    def get_function_definitions(
        self,
        provider: str = "openai"
    ) -> List[Dict[str, Any]]:
        """ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åˆ¥ã®é–¢æ•°å®šç¾©å–å¾—"""
        if provider == "openai":
            return [f.to_openai_format() for f in self.functions.values()]
        elif provider == "claude":
            return [f.to_claude_format() for f in self.functions.values()]
        else:
            raise ValueError(f"æœªå¯¾å¿œã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {provider}")

# ä½¿ç”¨ä¾‹
function_caller = FunctionCaller()

# é–¢æ•°å®šç¾©
get_weather_def = FunctionDefinition(
    name="get_weather",
    description="æŒ‡å®šã•ã‚ŒãŸå ´æ‰€ã®å¤©æ°—ã‚’å–å¾—",
    parameters={
        "type": "object",
        "properties": {
            "location": {
                "type": "string",
                "description": "éƒ½å¸‚åã€ä¾‹: æ±äº¬"
            },
            "unit": {
                "type": "string",
                "enum": ["celsius", "fahrenheit"],
                "description": "æ¸©åº¦å˜ä½"
            }
        },
        "required": ["location"]
    }
)

# ãƒãƒ³ãƒ‰ãƒ©ãƒ¼å®Ÿè£…
async def get_weather_handler(location: str, unit: str = "celsius") -> Dict[str, Any]:
    """å¤©æ°—æƒ…å ±å–å¾—ï¼ˆãƒ€ãƒŸãƒ¼å®Ÿè£…ï¼‰"""
    return {
        "location": location,
        "temperature": 22,
        "unit": unit,
        "description": "æ™´ã‚Œ"
    }

# ç™»éŒ²
function_caller.register_function(get_weather_def, get_weather_handler)
```

## ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†

### ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ‘ãƒ¼ã‚µãƒ¼

```python
class StreamingParser:
    """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¿œç­”ãƒ‘ãƒ¼ã‚µãƒ¼"""
    
    def __init__(self):
        self.buffer = ""
        self.current_function_call = None
        self.current_tool_calls = []
    
    async def parse_stream(
        self,
        stream: AsyncIterator[StreamChunk]
    ) -> AsyncIterator[Union[str, Dict[str, Any]]]:
        """ã‚¹ãƒˆãƒªãƒ¼ãƒ è§£æ"""
        async for chunk in stream:
            for choice in chunk.choices:
                delta = choice.delta
                
                # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
                if delta.content:
                    self.buffer += delta.content
                    yield delta.content
                
                # Function Call
                if delta.function_call:
                    if not self.current_function_call:
                        self.current_function_call = {
                            "name": delta.function_call.get("name", ""),
                            "arguments": ""
                        }
                    
                    if "arguments" in delta.function_call:
                        self.current_function_call["arguments"] += delta.function_call["arguments"]
                
                # Tool Calls
                if delta.tool_calls:
                    for tool_call_delta in delta.tool_calls:
                        # Tool Callå‡¦ç†
                        pass
                
                # å®Œäº†æ™‚
                if choice.finish_reason == "function_call":
                    if self.current_function_call:
                        # å¼•æ•°ã‚’JSONãƒ‘ãƒ¼ã‚¹
                        try:
                            args = json.loads(self.current_function_call["arguments"])
                            self.current_function_call["arguments"] = args
                            yield {"function_call": self.current_function_call}
                        except json.JSONDecodeError:
                            yield {"error": "Functionå¼•æ•°ã®ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼"}
                        
                        self.current_function_call = None

class EnhancedStreamingHandler:
    """æ‹¡å¼µã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒãƒ³ãƒ‰ãƒ©ãƒ¼"""
    
    def __init__(self, function_caller: Optional[FunctionCaller] = None):
        self.function_caller = function_caller
        self.parser = StreamingParser()
    
    async def handle_stream(
        self,
        stream: AsyncIterator[StreamChunk],
        callback: Optional[Callable] = None
    ) -> ChatCompletionResponse:
        """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ã¨ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        full_content = ""
        function_calls = []
        
        async for item in self.parser.parse_stream(stream):
            if isinstance(item, str):
                # ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„
                full_content += item
                if callback:
                    await callback({"type": "content", "data": item})
            
            elif isinstance(item, dict):
                if "function_call" in item:
                    # Function Call
                    function_call = item["function_call"]
                    function_calls.append(function_call)
                    
                    if callback:
                        await callback({"type": "function_call", "data": function_call})
                    
                    # é–¢æ•°å®Ÿè¡Œ
                    if self.function_caller:
                        result = await self.function_caller.execute_function_call(
                            function_call["name"],
                            function_call["arguments"]
                        )
                        
                        if callback:
                            await callback({"type": "function_result", "data": result})
        
        # æœ€çµ‚å¿œç­”ä½œæˆ
        message = Message(
            role=MessageRole.ASSISTANT,
            content=full_content,
            function_call=function_calls[0] if function_calls else None
        )
        
        return ChatCompletionResponse(
            id=f"stream-{datetime.utcnow().timestamp()}",
            model="streaming",
            created=datetime.utcnow(),
            choices=[Choice(index=0, message=message, finish_reason="stop")],
            usage=Usage(prompt_tokens=0, completion_tokens=0, total_tokens=0)
        )
```

## ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

### ã‚¨ãƒ©ãƒ¼ã‚¯ãƒ©ã‚¹å®šç¾©

```python
class AIError(Exception):
    """AIé–¢é€£ã‚¨ãƒ©ãƒ¼ã®åŸºåº•ã‚¯ãƒ©ã‚¹"""
    def __init__(self, message: str, code: Optional[str] = None, is_retryable: bool = False):
        super().__init__(message)
        self.code = code
        self.is_retryable = is_retryable

class RateLimitError(AIError):
    """ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼"""
    def __init__(self, message: str, retry_after: Optional[int] = None):
        super().__init__(message, code="rate_limit", is_retryable=True)
        self.retry_after = retry_after

class APIError(AIError):
    """APIã‚¨ãƒ©ãƒ¼"""
    pass

class AuthenticationError(AIError):
    """èªè¨¼ã‚¨ãƒ©ãƒ¼"""
    def __init__(self, message: str):
        super().__init__(message, code="authentication", is_retryable=False)

class InvalidRequestError(AIError):
    """ç„¡åŠ¹ãªãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼"""
    def __init__(self, message: str):
        super().__init__(message, code="invalid_request", is_retryable=False)

class MaxRetriesExceeded(AIError):
    """æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°è¶…é"""
    def __init__(self, message: str):
        super().__init__(message, code="max_retries", is_retryable=False)

class ErrorHandler:
    """çµ±ä¸€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒ©ãƒ¼"""
    
    @staticmethod
    def handle_provider_error(provider: str, error: Exception) -> AIError:
        """ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å›ºæœ‰ã‚¨ãƒ©ãƒ¼ã‚’çµ±ä¸€ã‚¨ãƒ©ãƒ¼ã«å¤‰æ›"""
        
        if provider == "openai":
            import openai
            if isinstance(error, openai.RateLimitError):
                return RateLimitError(str(error))
            elif isinstance(error, openai.AuthenticationError):
                return AuthenticationError(str(error))
            elif isinstance(error, openai.BadRequestError):
                return InvalidRequestError(str(error))
            elif isinstance(error, openai.APIError):
                return APIError(str(error), is_retryable=True)
        
        elif provider == "bedrock":
            from botocore.exceptions import ClientError
            if isinstance(error, ClientError):
                error_code = error.response['Error']['Code']
                if error_code == 'ThrottlingException':
                    return RateLimitError(str(error))
                elif error_code == 'AccessDeniedException':
                    return AuthenticationError(str(error))
                elif error_code == 'ValidationException':
                    return InvalidRequestError(str(error))
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        return APIError(str(error), is_retryable=False)
```

## ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–

### ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒã‚¿ãƒ¼å®Ÿè£…

```python
import time
from collections import deque
from dataclasses import dataclass
from typing import Deque

@dataclass
class RateLimitConfig:
    """ãƒ¬ãƒ¼ãƒˆåˆ¶é™è¨­å®š"""
    requests_per_minute: int = 60
    requests_per_hour: int = 3600
    tokens_per_minute: int = 90000
    tokens_per_hour: int = 2000000

class RateLimiter:
    """ãƒ¬ãƒ¼ãƒˆåˆ¶é™ç®¡ç†"""
    
    def __init__(self, config: RateLimitConfig = None):
        self.config = config or RateLimitConfig()
        self.request_times: Deque[float] = deque()
        self.token_usage: Deque[Tuple[float, int]] = deque()
    
    async def wait_if_needed(self):
        """å¿…è¦ã«å¿œã˜ã¦å¾…æ©Ÿ"""
        now = time.time()
        
        # å¤ã„ã‚¨ãƒ³ãƒˆãƒªã‚’å‰Šé™¤
        self._cleanup_old_entries(now)
        
        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆåˆ¶é™ãƒã‚§ãƒƒã‚¯
        minute_requests = self._count_recent_requests(now - 60)
        if minute_requests >= self.config.requests_per_minute:
            wait_time = 60 - (now - self.request_times[0])
            await asyncio.sleep(wait_time)
        
        # è¨˜éŒ²
        self.request_times.append(now)
    
    def record_token_usage(self, tokens: int):
        """ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡è¨˜éŒ²"""
        self.token_usage.append((time.time(), tokens))
    
    def _cleanup_old_entries(self, now: float):
        """å¤ã„ã‚¨ãƒ³ãƒˆãƒªå‰Šé™¤"""
        hour_ago = now - 3600
        
        while self.request_times and self.request_times[0] < hour_ago:
            self.request_times.popleft()
        
        while self.token_usage and self.token_usage[0][0] < hour_ago:
            self.token_usage.popleft()
    
    def _count_recent_requests(self, since: float) -> int:
        """æœ€è¿‘ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°"""
        return sum(1 for t in self.request_times if t >= since)
    
    def _count_recent_tokens(self, since: float) -> int:
        """æœ€è¿‘ã®ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡"""
        return sum(tokens for t, tokens in self.token_usage if t >= since)

class AdaptiveRateLimiter(RateLimiter):
    """é©å¿œå‹ãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒã‚¿ãƒ¼"""
    
    def __init__(self, config: RateLimitConfig = None):
        super().__init__(config)
        self.success_rate = 1.0
        self.backoff_multiplier = 1.0
    
    async def wait_if_needed(self):
        """é©å¿œçš„ãªå¾…æ©Ÿ"""
        # åŸºæœ¬å¾…æ©Ÿ
        await super().wait_if_needed()
        
        # æˆåŠŸç‡ã«åŸºã¥ãè¿½åŠ å¾…æ©Ÿ
        if self.success_rate < 0.8:
            additional_wait = (1 - self.success_rate) * 10 * self.backoff_multiplier
            await asyncio.sleep(additional_wait)
    
    def record_success(self):
        """æˆåŠŸè¨˜éŒ²"""
        self.success_rate = min(1.0, self.success_rate + 0.1)
        self.backoff_multiplier = max(1.0, self.backoff_multiplier - 0.1)
    
    def record_failure(self):
        """å¤±æ•—è¨˜éŒ²"""
        self.success_rate = max(0.0, self.success_rate - 0.2)
        self.backoff_multiplier = min(5.0, self.backoff_multiplier + 0.5)
```

## ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç®¡ç†

### ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼

```python
@dataclass
class PromptTemplate:
    """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ"""
    name: str
    template: str
    variables: List[str]
    system_prompt: Optional[str] = None
    examples: List[Dict[str, str]] = None
    
    def format(self, **kwargs) -> str:
        """å¤‰æ•°ã‚’åŸ‹ã‚è¾¼ã‚“ã§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ"""
        # ä¸è¶³ã—ã¦ã„ã‚‹å¤‰æ•°ãƒã‚§ãƒƒã‚¯
        missing = set(self.variables) - set(kwargs.keys())
        if missing:
            raise ValueError(f"å¿…é ˆå¤‰æ•°ãŒä¸è¶³: {missing}")
        
        return self.template.format(**kwargs)

class PromptManager:
    """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç®¡ç†"""
    
    def __init__(self):
        self.templates: Dict[str, PromptTemplate] = {}
        self._load_default_templates()
    
    def _load_default_templates(self):
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆèª­ã¿è¾¼ã¿"""
        
        # MAKOTOãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        self.templates["makoto_chat"] = PromptTemplate(
            name="makoto_chat",
            system_prompt="""ã‚ãªãŸã¯MAKOTOï¼ˆãƒã‚³ãƒˆï¼‰ã¨ã„ã†åå‰ã®AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®æ€§æ ¼ã¨ç‰¹å¾´ã‚’æŒã£ã¦ã„ã¾ã™ï¼š
- è¦ªåˆ‡ã§ä¸å¯§ãªå¯¾å¿œ
- å°‚é–€çš„ãªçŸ¥è­˜ã‚’åˆ†ã‹ã‚Šã‚„ã™ãèª¬æ˜
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ„Ÿæƒ…ã«å¯„ã‚Šæ·»ã†
- å¿…è¦ã«å¿œã˜ã¦çµµæ–‡å­—ã‚’ä½¿ç”¨ï¼ˆæ§ãˆã‚ã«ï¼‰

å¿œç­”æ™‚ã¯ä»¥ä¸‹ã®å½¢å¼ã§JSONã‚’è¿”ã—ã¦ãã ã•ã„ï¼š
{
    "content": "å¿œç­”å†…å®¹",
    "emotion": "neutral|happy|sad|angry|surprised|confused|thinking",
    "metadata": {
        "confidence": 0.0-1.0,
        "tags": []
    }
}""",
            template="{user_message}",
            variables=["user_message"]
        )
        
        # è¦ç´„ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        self.templates["summarize"] = PromptTemplate(
            name="summarize",
            template="""ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’{max_length}æ–‡å­—ä»¥å†…ã§è¦ç´„ã—ã¦ãã ã•ã„ï¼š

{text}

è¦ç´„ï¼š""",
            variables=["text", "max_length"]
        )
        
        # ç¿»è¨³ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        self.templates["translate"] = PromptTemplate(
            name="translate",
            template="""ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’{source_lang}ã‹ã‚‰{target_lang}ã«ç¿»è¨³ã—ã¦ãã ã•ã„ï¼š

{text}

ç¿»è¨³ï¼š""",
            variables=["text", "source_lang", "target_lang"]
        )
    
    def get_template(self, name: str) -> PromptTemplate:
        """ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå–å¾—"""
        if name not in self.templates:
            raise ValueError(f"ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {name}")
        return self.templates[name]
    
    def register_template(self, template: PromptTemplate):
        """ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç™»éŒ²"""
        self.templates[template.name] = template
    
    def create_messages(
        self,
        template_name: str,
        **kwargs
    ) -> List[Message]:
        """ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‹ã‚‰ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä½œæˆ"""
        template = self.get_template(template_name)
        
        messages = []
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        if template.system_prompt:
            messages.append(Message(
                role=MessageRole.SYSTEM,
                content=template.system_prompt
            ))
        
        # ä¾‹ç¤º
        if template.examples:
            for example in template.examples:
                messages.append(Message(
                    role=MessageRole.USER,
                    content=example["input"]
                ))
                messages.append(Message(
                    role=MessageRole.ASSISTANT,
                    content=example["output"]
                ))
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        messages.append(Message(
            role=MessageRole.USER,
            content=template.format(**kwargs)
        ))
        
        return messages
```

## ãƒ¢ãƒ‡ãƒ«ç®¡ç†

### ãƒ¢ãƒ‡ãƒ«ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼

```python
@dataclass
class ModelInfo:
    """ãƒ¢ãƒ‡ãƒ«æƒ…å ±"""
    id: str
    provider: str
    name: str
    context_length: int
    max_output_tokens: int
    cost_per_1k_input: float
    cost_per_1k_output: float
    capabilities: List[str]  # ["chat", "function_calling", "vision", etc.]
    
    def supports(self, capability: str) -> bool:
        """æ©Ÿèƒ½ã‚µãƒãƒ¼ãƒˆç¢ºèª"""
        return capability in self.capabilities

class ModelManager:
    """ãƒ¢ãƒ‡ãƒ«ç®¡ç†"""
    
    def __init__(self):
        self.models: Dict[str, ModelInfo] = {}
        self._load_model_definitions()
    
    def _load_model_definitions(self):
        """ãƒ¢ãƒ‡ãƒ«å®šç¾©èª­ã¿è¾¼ã¿"""
        
        # OpenAI ãƒ¢ãƒ‡ãƒ«
        self.models["gpt-4-turbo"] = ModelInfo(
            id="gpt-4-turbo",
            provider="openai",
            name="GPT-4 Turbo",
            context_length=128000,
            max_output_tokens=4096,
            cost_per_1k_input=0.01,
            cost_per_1k_output=0.03,
            capabilities=["chat", "function_calling", "vision", "json_mode"]
        )
        
        self.models["gpt-4o"] = ModelInfo(
            id="gpt-4o",
            provider="openai",
            name="GPT-4o",
            context_length=128000,
            max_output_tokens=4096,
            cost_per_1k_input=0.005,
            cost_per_1k_output=0.015,
            capabilities=["chat", "function_calling", "vision", "json_mode"]
        )
        
        # Azure OpenAI ãƒ¢ãƒ‡ãƒ«
        self.models["gpt-4-azure"] = ModelInfo(
            id="gpt-4",
            provider="azure",
            name="GPT-4 (Azure)",
            context_length=8192,
            max_output_tokens=4096,
            cost_per_1k_input=0.03,
            cost_per_1k_output=0.06,
            capabilities=["chat", "function_calling"]
        )
        
        # Bedrock ãƒ¢ãƒ‡ãƒ«
        self.models["claude-3-sonnet"] = ModelInfo(
            id="anthropic.claude-3-sonnet-20240229-v1:0",
            provider="bedrock",
            name="Claude 3 Sonnet",
            context_length=200000,
            max_output_tokens=4096,
            cost_per_1k_input=0.003,
            cost_per_1k_output=0.015,
            capabilities=["chat", "vision"]
        )
    
    def get_model(self, model_id: str) -> ModelInfo:
        """ãƒ¢ãƒ‡ãƒ«æƒ…å ±å–å¾—"""
        if model_id not in self.models:
            raise ValueError(f"æœªç™»éŒ²ã®ãƒ¢ãƒ‡ãƒ«: {model_id}")
        return self.models[model_id]
    
    def select_model(
        self,
        required_capabilities: List[str],
        max_cost_per_1k: float = None,
        min_context_length: int = None
    ) -> ModelInfo:
        """æ¡ä»¶ã«åˆã†ãƒ¢ãƒ‡ãƒ«é¸æŠ"""
        candidates = []
        
        for model in self.models.values():
            # å¿…è¦ãªæ©Ÿèƒ½ã‚’ã‚µãƒãƒ¼ãƒˆ
            if not all(model.supports(cap) for cap in required_capabilities):
                continue
            
            # ã‚³ã‚¹ãƒˆåˆ¶ç´„
            if max_cost_per_1k and model.cost_per_1k_input > max_cost_per_1k:
                continue
            
            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·
            if min_context_length and model.context_length < min_context_length:
                continue
            
            candidates.append(model)
        
        if not candidates:
            raise ValueError("æ¡ä»¶ã«åˆã†ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        # ã‚³ã‚¹ãƒˆãŒæœ€ã‚‚ä½ã„ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
        return min(candidates, key=lambda m: m.cost_per_1k_input)
```

## ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†

### ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼

```python
@dataclass
class RequestMetrics:
    """ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    timestamp: datetime
    provider: str
    model: str
    input_tokens: int
    output_tokens: int
    total_tokens: int
    latency_ms: float
    cost: float
    success: bool
    error: Optional[str] = None

class MetricsCollector:
    """ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†"""
    
    def __init__(self):
        self.metrics: List[RequestMetrics] = []
        self.aggregated_metrics = {}
    
    async def record_request(
        self,
        provider: str,
        model: str,
        tokens: int,
        latency: float,
        success: bool = True,
        error: Optional[str] = None
    ):
        """ãƒªã‚¯ã‚¨ã‚¹ãƒˆè¨˜éŒ²"""
        # ãƒ¢ãƒ‡ãƒ«æƒ…å ±å–å¾—
        model_manager = ModelManager()
        model_info = model_manager.get_model(model)
        
        # ã‚³ã‚¹ãƒˆè¨ˆç®—
        cost = (tokens / 1000) * model_info.cost_per_1k_input
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä½œæˆ
        metric = RequestMetrics(
            timestamp=datetime.utcnow(),
            provider=provider,
            model=model,
            input_tokens=0,  # å®Ÿè£…ã§åˆ†é›¢
            output_tokens=0,
            total_tokens=tokens,
            latency_ms=latency * 1000,
            cost=cost,
            success=success,
            error=error
        )
        
        self.metrics.append(metric)
        
        # é›†è¨ˆæ›´æ–°
        await self._update_aggregates(metric)
    
    async def _update_aggregates(self, metric: RequestMetrics):
        """é›†è¨ˆãƒ‡ãƒ¼ã‚¿æ›´æ–°"""
        key = f"{metric.provider}:{metric.model}"
        
        if key not in self.aggregated_metrics:
            self.aggregated_metrics[key] = {
                "total_requests": 0,
                "total_tokens": 0,
                "total_cost": 0,
                "total_latency": 0,
                "success_count": 0,
                "error_count": 0
            }
        
        agg = self.aggregated_metrics[key]
        agg["total_requests"] += 1
        agg["total_tokens"] += metric.total_tokens
        agg["total_cost"] += metric.cost
        agg["total_latency"] += metric.latency_ms
        
        if metric.success:
            agg["success_count"] += 1
        else:
            agg["error_count"] += 1
    
    def get_statistics(
        self,
        provider: Optional[str] = None,
        model: Optional[str] = None,
        since: Optional[datetime] = None
    ) -> Dict[str, Any]:
        """çµ±è¨ˆæƒ…å ±å–å¾—"""
        filtered_metrics = self.metrics
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        if provider:
            filtered_metrics = [m for m in filtered_metrics if m.provider == provider]
        if model:
            filtered_metrics = [m for m in filtered_metrics if m.model == model]
        if since:
            filtered_metrics = [m for m in filtered_metrics if m.timestamp >= since]
        
        if not filtered_metrics:
            return {}
        
        # çµ±è¨ˆè¨ˆç®—
        total_requests = len(filtered_metrics)
        total_tokens = sum(m.total_tokens for m in filtered_metrics)
        total_cost = sum(m.cost for m in filtered_metrics)
        avg_latency = sum(m.latency_ms for m in filtered_metrics) / total_requests
        success_rate = sum(1 for m in filtered_metrics if m.success) / total_requests
        
        return {
            "total_requests": total_requests,
            "total_tokens": total_tokens,
            "total_cost": total_cost,
            "average_latency_ms": avg_latency,
            "success_rate": success_rate,
            "period": {
                "start": min(m.timestamp for m in filtered_metrics),
                "end": max(m.timestamp for m in filtered_metrics)
            }
        }
```

## ãƒ†ã‚¹ãƒˆæˆ¦ç•¥

### ãƒ¢ãƒƒã‚¯ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ

```python
class MockAIClient(AIClientBase):
    """ãƒ†ã‚¹ãƒˆç”¨ãƒ¢ãƒƒã‚¯ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.responses = []
        self.response_index = 0
    
    def add_response(self, response: ChatCompletionResponse):
        """ãƒ¬ã‚¹ãƒãƒ³ã‚¹è¿½åŠ """
        self.responses.append(response)
    
    async def create_chat_completion(
        self,
        request: ChatCompletionRequest
    ) -> ChatCompletionResponse:
        """ãƒ¢ãƒƒã‚¯å¿œç­”"""
        if self.response_index >= len(self.responses):
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå¿œç­”
            return ChatCompletionResponse(
                id="mock-response",
                model=request.model,
                created=datetime.utcnow(),
                choices=[Choice(
                    index=0,
                    message=Message(
                        role=MessageRole.ASSISTANT,
                        content="ãƒ¢ãƒƒã‚¯å¿œç­”ã§ã™"
                    ),
                    finish_reason="stop"
                )],
                usage=Usage(
                    prompt_tokens=10,
                    completion_tokens=5,
                    total_tokens=15
                )
            )
        
        response = self.responses[self.response_index]
        self.response_index += 1
        return response
    
    async def create_chat_completion_stream(
        self,
        request: ChatCompletionRequest
    ) -> AsyncIterator[StreamChunk]:
        """ãƒ¢ãƒƒã‚¯ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°"""
        content = "ã“ã‚Œã¯ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ†ã‚¹ãƒˆã§ã™"
        
        for char in content:
            yield StreamChunk(
                id="mock-stream",
                model=request.model,
                created=datetime.utcnow(),
                choices=[StreamChoice(
                    index=0,
                    delta=Delta(content=char),
                    finish_reason=None
                )]
            )
        
        # æœ€çµ‚ãƒãƒ£ãƒ³ã‚¯
        yield StreamChunk(
            id="mock-stream",
            model=request.model,
            created=datetime.utcnow(),
            choices=[StreamChoice(
                index=0,
                delta=Delta(),
                finish_reason="stop"
            )]
        )
    
    async def count_tokens(
        self,
        messages: List[Message],
        model: str
    ) -> int:
        """ãƒ¢ãƒƒã‚¯ãƒˆãƒ¼ã‚¯ãƒ³è¨ˆç®—"""
        return sum(len(m.content) for m in messages) // 4

# ãƒ†ã‚¹ãƒˆä¾‹
import pytest

@pytest.mark.asyncio
async def test_ai_client():
    """AIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãƒ†ã‚¹ãƒˆ"""
    # ãƒ¢ãƒƒã‚¯ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
    client = MockAIClient()
    
    # ãƒ†ã‚¹ãƒˆå¿œç­”
    test_response = ChatCompletionResponse(
        id="test-1",
        model="gpt-4",
        created=datetime.utcnow(),
        choices=[Choice(
            index=0,
            message=Message(
                role=MessageRole.ASSISTANT,
                content="ãƒ†ã‚¹ãƒˆå¿œç­”"
            ),
            finish_reason="stop"
        )],
        usage=Usage(10, 5, 15)
    )
    
    client.add_response(test_response)
    
    # ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
    request = ChatCompletionRequest(
        messages=[Message(role=MessageRole.USER, content="ãƒ†ã‚¹ãƒˆ")],
        model="gpt-4"
    )
    
    # å®Ÿè¡Œ
    response = await client.create_chat_completion(request)
    
    # æ¤œè¨¼
    assert response.id == "test-1"
    assert response.choices[0].message.content == "ãƒ†ã‚¹ãƒˆå¿œç­”"
    assert response.usage.total_tokens == 15

@pytest.mark.asyncio
async def test_streaming():
    """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ†ã‚¹ãƒˆ"""
    client = MockAIClient()
    
    request = ChatCompletionRequest(
        messages=[Message(role=MessageRole.USER, content="ãƒ†ã‚¹ãƒˆ")],
        model="gpt-4",
        stream=True
    )
    
    # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å®Ÿè¡Œ
    chunks = []
    async for chunk in client.create_chat_completion_stream(request):
        chunks.append(chunk)
    
    # æ¤œè¨¼
    assert len(chunks) > 0
    content = "".join(
        c.choices[0].delta.content 
        for c in chunks 
        if c.choices[0].delta.content
    )
    assert content == "ã“ã‚Œã¯ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ†ã‚¹ãƒˆã§ã™"
```

## ã¾ã¨ã‚

ã“ã®AIå¿œç­”æ¨™æº–åŒ–ã‚¬ã‚¤ãƒ‰ã«ã‚ˆã‚Šï¼š

1. **çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹**: ç•°ãªã‚‹LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’åŒã˜APIã§æ“ä½œ
2. **Function Calling**: ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼é–“ã§Function Callingã‚’æ¨™æº–åŒ–
3. **ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¿œç­”ã®çµ±ä¸€å‡¦ç†
4. **ã‚¨ãƒ©ãƒ¼å‡¦ç†**: ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å›ºæœ‰ã‚¨ãƒ©ãƒ¼ã®çµ±ä¸€åŒ–
5. **ãƒ¬ãƒ¼ãƒˆåˆ¶é™**: é©å¿œçš„ãªãƒ¬ãƒ¼ãƒˆåˆ¶é™ç®¡ç†
6. **ãƒ¡ãƒˆãƒªã‚¯ã‚¹**: ä½¿ç”¨çŠ¶æ³ã®è©³ç´°ãªè¿½è·¡
7. **ãƒ†ã‚¹ã‚¿ãƒ“ãƒªãƒ†ã‚£**: ãƒ¢ãƒƒã‚¯ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«ã‚ˆã‚‹å®¹æ˜“ãªãƒ†ã‚¹ãƒˆ

ã“ã‚Œã«ã‚ˆã‚Šã€LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®åˆ‡ã‚Šæ›¿ãˆãŒå®¹æ˜“ã«ãªã‚Šã€ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ä¿å®ˆæ€§ã¨æ‹¡å¼µæ€§ãŒå‘ä¸Šã—ã¾ã™ã€‚

---

**ä½œæˆè€…**: Claude  
**ä½œæˆæ—¥**: 2025å¹´8æœˆ7æ—¥  
**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.0.0